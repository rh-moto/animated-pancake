{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02049c5d",
   "metadata": {},
   "source": [
    "# Load Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3901dec4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# load modules\n",
    "import os\n",
    "import re\n",
    "import itertools\n",
    "import Haver\n",
    "import calendar\n",
    "import blpapi\n",
    "# import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pyarrow as pa\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas.io.formats.excel\n",
    "from tqdm.notebook import tqdm\n",
    "# from openpyxl import load_workbook\n",
    "from datetime import datetime\n",
    "from dateutil import relativedelta\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from scipy.stats import norminvgauss, wald\n",
    "from xbbg import blp\n",
    "\n",
    "pd.io.formats.excel.ExcelFormatter.header_style = None\n",
    "writer_spec = {\n",
    "    \"engine\":\"openpyxl\",\n",
    "    \"mode\":\"a\",\n",
    "    \"if_sheet_exists\":\"overlay\"\n",
    "}\n",
    "\n",
    "def using_clump(a):\n",
    "    return [a[s] for s in np.ma.clump_unmasked(np.ma.masked_invalid(a))]\n",
    "\n",
    "sessionOptions = blpapi.SessionOptions()\n",
    "sessionOptions.setApplicationIdentityKey(\"52e02b77-2aa7-4412-b718-804f115a0546\")\n",
    "session = blpapi.Session(sessionOptions)\n",
    "kwargs = {'sess':session}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d2965e",
   "metadata": {},
   "source": [
    "# Historical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff9ef24",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "itemList = pd.read_excel('./cpi_itemList.xlsx', sheet_name='itemList')\n",
    "itemList_dict = itemList.set_index('ItemCode').to_dict()\n",
    "startMonth = datetime(1970,1,1)\n",
    "# endMonth = datetime(2025,3,1)\n",
    "today = datetime.today()\n",
    "\n",
    "#全国だから前月初\n",
    "endMonth = today - relativedelta.relativedelta(months=1)\n",
    "endMonth = endMonth.replace(day=1)\n",
    "\n",
    "nhor = (endMonth.year - startMonth.year)*12 + endMonth.month - startMonth.month + 1\n",
    "darray = np.full((nhor, len(itemList)), fill_value=np.nan)\n",
    "temp_df = pd.DataFrame(data=darray, columns=itemList['ItemCode'])\n",
    "temp_df.index = [startMonth + relativedelta.relativedelta(months=x) for x in range(nhor)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb33991",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "all_dfs = pd.read_excel('./all_data.xlsx', engine=\"openpyxl\", sheet_name=None, index_col=0, skiprows=[0,1,3,4,5])\n",
    "itemMonthWeights = pd.read_excel('./monthWeights.xlsx', engine=\"openpyxl\", sheet_name=None)\n",
    "yoy_adjustments = pd.read_excel('./cpi_special_factor.xlsx', engine=\"openpyxl\", sheet_name=None, index_col=0, skiprows=[0,1,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0187c290",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# yoy\n",
    "df_yoy = temp_df.copy()\n",
    "for ix, code in enumerate(itemList['ItemCode']):\n",
    "    if itemList['in2020'][ix] == 1:\n",
    "        df_yoy[code] = all_dfs['20yoy'][code]\n",
    "    elif itemList['in2015'][ix] == 1:\n",
    "        df_yoy[code] = all_dfs['15yoy'][code]\n",
    "    elif itemList['in2010'][ix] == 1:\n",
    "        df_yoy[code] = all_dfs['10yoy'][code]\n",
    "    else:\n",
    "        df_yoy[code] = all_dfs['05yoy'][code]\n",
    "df_yoy.columns = pd.MultiIndex.from_arrays(itemList[['ItemCode','Name','Cat','Type']].T.values)\n",
    "\n",
    "# interpolate seasonal survey items (yoy)\n",
    "for col in df_yoy.columns:\n",
    "    test_array = df_yoy[col].values\n",
    "    list_by_nan = using_clump(test_array)\n",
    "    if len(list_by_nan) > 1 and col[2] != 1:\n",
    "        df_intplt = df_yoy[[col]].copy()\n",
    "        avg_list = [x.mean() for x in list_by_nan]\n",
    "        df_intplt['NAflag'] = df_intplt[col].isna()*1\n",
    "        df_intplt['L_NAflag'] = df_intplt['NAflag'].shift(1)\n",
    "        df_intplt['NA_end'] = [1 if x==1 and y==0 else 0 for x,y in zip(df_intplt['NAflag'],df_intplt['L_NAflag'])]\n",
    "        df_intplt['NA_cum'] = df_intplt['NA_end'].cumsum()*df_intplt['NAflag']\n",
    "\n",
    "        df_intplt['fillflag'] = 0\n",
    "        if itemList_dict['in2005'][col[0]] == 1:\n",
    "            df_intplt['fillflag'] = [1 if y < datetime(2011,1,1) else x for x,y in zip(df_intplt['fillflag'],df_intplt.index)]\n",
    "        elif itemList_dict['in2005r'][col[0]] == 1:\n",
    "            df_intplt['fillflag'] = [1 if y < datetime(2011,1,1) else x for x,y in zip(df_intplt['fillflag'],df_intplt.index)]\n",
    "        elif itemList_dict['in2010'][col[0]] == 1:\n",
    "            df_intplt['fillflag'] = [1 if y < datetime(2016,1,1) else x for x,y in zip(df_intplt['fillflag'],df_intplt.index)]\n",
    "        elif itemList_dict['in2015'][col[0]] == 1:\n",
    "            df_intplt['fillflag'] = [1 if y < datetime(2021,1,1) else x for x,y in zip(df_intplt['fillflag'],df_intplt.index)]\n",
    "        elif itemList_dict['in2020'][col[0]] == 1:\n",
    "            df_intplt['fillflag'] = [1 if y < datetime(2026,1,1) else x for x,y in zip(df_intplt['fillflag'],df_intplt.index)]\n",
    "\n",
    "        df_intplt['newcol'] = [x if y==0 else avg_list[y-1] if z==1 else np.nan for x,y,z in zip(df_intplt[col],df_intplt['NA_cum'],df_intplt['fillflag'])]\n",
    "        df_yoy[col] = df_intplt['newcol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d24c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index\n",
    "df_index = temp_df.copy()\n",
    "for ix, code in enumerate(itemList['ItemCode']):\n",
    "    if itemList['in2020'][ix] == 1:\n",
    "        df_index[code] = all_dfs['20index'][code]\n",
    "    elif itemList['in2015'][ix] == 1:\n",
    "        df_index[code] = all_dfs['15index'][code]\n",
    "    elif itemList['in2010'][ix] == 1:\n",
    "        df_index[code] = all_dfs['10index'][code]\n",
    "    else:\n",
    "        df_index[code] = all_dfs['05index'][code]\n",
    "df_index.columns = pd.MultiIndex.from_arrays(itemList[['ItemCode','Name','Cat','Type']].T.values)\n",
    "\n",
    "# interpolate seasonal survey items (index)\n",
    "for col in df_index.columns:\n",
    "    test_array = df_index[col].values\n",
    "    list_by_nan = using_clump(test_array)\n",
    "    if len(list_by_nan) > 1 and col[2] != 1:\n",
    "        df_intplt = df_index[[col]].copy()\n",
    "        avg_list = [x.mean() for x in list_by_nan]\n",
    "        df_intplt['NAflag'] = df_intplt[col].isna()*1\n",
    "        df_intplt['L_NAflag'] = df_intplt['NAflag'].shift(1)\n",
    "        df_intplt['NA_end'] = [1 if x==1 and y==0 else 0 for x,y in zip(df_intplt['NAflag'],df_intplt['L_NAflag'])]\n",
    "        df_intplt['NA_cum'] = df_intplt['NA_end'].cumsum()*df_intplt['NAflag']\n",
    "        df_intplt['newcol'] = [x if y==0 else avg_list[y-1] for x,y in zip(df_intplt[col], df_intplt['NA_cum'])]\n",
    "        df_index[col] = df_intplt['newcol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769e84f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to set weight\n",
    "itemFixWeights = itemList[['ItemCode','weight20','weight15','weight10','weight05r','weight05']].set_index('ItemCode').to_dict()\n",
    "def set_weight(i, idx, col, itemFixWeights=itemFixWeights, itemMonthWeights=itemMonthWeights, df_yoy=df_yoy, df_index=df_index, type='yoy'):\n",
    "    itemCode = col[0]\n",
    "    itemName = col[1]\n",
    "    if type == 'yoy':\n",
    "        na_check = df_yoy[col].isna()\n",
    "        if na_check.iloc[i]:\n",
    "            return None\n",
    "        else:\n",
    "            if idx <= datetime(2008, 12, 31):\n",
    "                if itemName in itemMonthWeights['05mwght'].columns:\n",
    "                    # print(itemName, idx)\n",
    "                    return itemMonthWeights['05mwght'][itemName][idx.month-1]\n",
    "                else:\n",
    "                    return itemFixWeights['weight05'][itemCode]\n",
    "            elif idx <= datetime(2010, 12, 31):\n",
    "                if itemName in itemMonthWeights['05mwght'].columns:\n",
    "                    return itemMonthWeights['05mwght'][itemName][idx.month-1]\n",
    "                else:\n",
    "                    return itemFixWeights['weight05r'][itemCode]\n",
    "            elif idx <= datetime(2015, 12, 31):\n",
    "                if itemName in itemMonthWeights['10mwght'].columns:\n",
    "                    return itemMonthWeights['10mwght'][itemName][idx.month-1]\n",
    "                else:\n",
    "                    return itemFixWeights['weight10'][itemCode]\n",
    "            elif idx <= datetime(2020, 12, 31):\n",
    "                if itemName in itemMonthWeights['15mwght'].columns:\n",
    "                    return itemMonthWeights['15mwght'][itemName][idx.month-1]\n",
    "                else:\n",
    "                    return itemFixWeights['weight15'][itemCode]\n",
    "            else:\n",
    "                if itemName in itemMonthWeights['20mwght'].columns:\n",
    "                    return itemMonthWeights['20mwght'][itemName][idx.month-1]\n",
    "                else:\n",
    "                    return itemFixWeights['weight20'][itemCode]\n",
    "    else:\n",
    "        na_check = df_index[col].isna()\n",
    "        if na_check.iloc[i]:\n",
    "            return None\n",
    "        else:\n",
    "            if idx <= datetime(2007, 12, 31):\n",
    "                if itemName in itemMonthWeights['05mwght'].columns:\n",
    "                    return itemMonthWeights['05mwght'][itemName][idx.month-1]\n",
    "                else:\n",
    "                    return itemFixWeights['weight05'][itemCode]\n",
    "            elif idx <= datetime(2009, 12, 31):\n",
    "                if itemName in itemMonthWeights['05mwght'].columns:\n",
    "                    return itemMonthWeights['05mwght'][itemName][idx.month-1]\n",
    "                else:\n",
    "                    return itemFixWeights['weight05r'][itemCode]\n",
    "            elif idx <= datetime(2014, 12, 31):\n",
    "                if itemName in itemMonthWeights['10mwght'].columns:\n",
    "                    return itemMonthWeights['10mwght'][itemName][idx.month-1]\n",
    "                else:\n",
    "                    return itemFixWeights['weight10'][itemCode]\n",
    "            elif idx <= datetime(2019, 12, 31):\n",
    "                if itemName in itemMonthWeights['15mwght'].columns:\n",
    "                    return itemMonthWeights['15mwght'][itemName][idx.month-1]\n",
    "                else:\n",
    "                    return itemFixWeights['weight15'][itemCode]\n",
    "            else:\n",
    "                if itemName in itemMonthWeights['20mwght'].columns:\n",
    "                    return itemMonthWeights['20mwght'][itemName][idx.month-1]\n",
    "                else:\n",
    "                    return itemFixWeights['weight20'][itemCode]\n",
    "\n",
    "\n",
    "# weight (yoy)\n",
    "df_weight_yoy = temp_df.copy()\n",
    "df_weight_yoy.columns = pd.MultiIndex.from_arrays(itemList[['ItemCode','Name','Cat','Type']].T.values)\n",
    "for col in tqdm(df_weight_yoy.columns):\n",
    "    df_weight_yoy[col] = [set_weight(i, idx, col) for i, idx in enumerate(df_weight_yoy.index)]\n",
    "\n",
    "# weight (index)\n",
    "df_weight_index = temp_df.copy()\n",
    "df_weight_index.columns = pd.MultiIndex.from_arrays(itemList[['ItemCode','Name','Cat','Type']].T.values)\n",
    "for col in tqdm(df_weight_index.columns):\n",
    "    df_weight_index[col] = [set_weight(i, idx, col, type='index') for i, idx in enumerate(df_weight_index.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af84c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yoy excluding special factors (VAT, energy, education, travel)\n",
    "yoy_adjustments = pd.read_excel('./cpi_special_factor.xlsx', engine=\"openpyxl\", sheet_name=None, index_col=0, header=[0,1,2,3])\n",
    "array_yoy_adj = df_yoy.values\n",
    "for key in yoy_adjustments.keys():\n",
    "    adj_array = yoy_adjustments[key].values\n",
    "    adj_array = np.nan_to_num(adj_array)\n",
    "    array_yoy_adj = array_yoy_adj + adj_array\n",
    "\n",
    "df_yoy_adj = pd.DataFrame(\n",
    "    data=array_yoy_adj,\n",
    "    columns=df_yoy.columns,\n",
    "    index=df_yoy.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bb22be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save\n",
    "dfs = {\n",
    "    'yoy':df_yoy,\n",
    "    'index':df_index,\n",
    "    'yoy_weight':df_weight_yoy,\n",
    "    'index_weight':df_weight_index,\n",
    "    'yoy_adj':df_yoy_adj\n",
    "}\n",
    "with pd.ExcelWriter('./cpi_all.xlsx') as writer:\n",
    "    for df_name, df in dfs.items():\n",
    "        df.to_excel(writer, sheet_name=df_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dd4494",
   "metadata": {},
   "source": [
    "# CPI (special factors adjusted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529af3e4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "cpi_dfs = pd.read_excel('./cpi_all.xlsx', engine=\"openpyxl\", sheet_name=None, index_col=0, skiprows=[0,2,3,4])\n",
    "df_cpi_all = pd.DataFrame()\n",
    "for ix, key in enumerate(cpi_dfs.keys()):\n",
    "    temp_df = cpi_dfs[key].reset_index().melt(id_vars='index')\n",
    "    temp_df.columns = ['Date','Name',key]\n",
    "\n",
    "    if ix == 0:\n",
    "        df_cpi_all = temp_df.copy()\n",
    "    else:\n",
    "        df_cpi_all[key] = temp_df[key]\n",
    "\n",
    "meta_cols = [\n",
    "    'ItemCode',\n",
    "    'Name',\n",
    "    'Cat',\n",
    "    'Type',\n",
    "    'Overall',\n",
    "    'Core',\n",
    "    'BoJCore',\n",
    "    'WesternCore',\n",
    "    'CPIExImpRents',\n",
    "    'CPIExImpRents&FF',\n",
    "    'GoodsExFFE',\n",
    "    'GenSerExRents',\n",
    "    'Ser',\n",
    "    'Foods',\n",
    "    'ImpRents',\n",
    "    'Energy',\n",
    "    'FreshFoods',\n",
    "    'freq_cat',\n",
    "]\n",
    "df_cpi_all = df_cpi_all.merge(itemList[meta_cols], on='Name', how='left')\n",
    "df_cpi_all['Atlanta'] = ['Flexible' if x == 'High' else 'Sticky' for x in df_cpi_all['freq_cat']]\n",
    "df_cpi_all.to_parquet('./df_cpi_all.parquet', compression='zstd')\n",
    "df_cpi_all.to_csv('./df_cpi_all.csv', encoding='shift-jis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f28547",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# calculate yoy\n",
    "df_cpi_all['yoy:weight'] = df_cpi_all['yoy_adj']*df_cpi_all['yoy_weight']\n",
    "cpi_yoy_df = pd.DataFrame(index=cpi_dfs['yoy'].index)\n",
    "\n",
    "# Overall/Core/BoJ core/CPI ex imputed rents and FF\n",
    "cal_cols = [\n",
    "    'Overall',\n",
    "    'Core',\n",
    "    'BoJCore',\n",
    "    'WesternCore',\n",
    "    'CPIExImpRents',\n",
    "    'CPIExImpRents&FF',\n",
    "    'GoodsExFFE',\n",
    "    'GenSerExRents',\n",
    "    'Ser',\n",
    "    'Foods',\n",
    "    'ImpRents',\n",
    "    'Energy',\n",
    "    'FreshFoods',\n",
    "]\n",
    "\n",
    "for col in cal_cols:\n",
    "    temp_df = df_cpi_all.copy()[df_cpi_all[col] == 1]\n",
    "    temp_agg_yoy = temp_df.groupby(['Date'])[['yoy_weight','yoy:weight']].sum()\n",
    "    temp_agg_yoy[col] = temp_agg_yoy['yoy:weight']/temp_agg_yoy['yoy_weight']\n",
    "    cpi_yoy_df[col] = temp_agg_yoy[col]\n",
    "\n",
    "cpi_yoy_df.dropna(inplace=True, how='all')\n",
    "\n",
    "# aggregate by category\n",
    "cat_cols = ['Type','Cat','freq_cat','Atlanta']\n",
    "for cat_col in cat_cols:\n",
    "    temp_df = df_cpi_all.groupby(['Date',cat_col])[['yoy_weight','yoy:weight']].sum()\n",
    "    temp_df['yoy'] = temp_df['yoy:weight']/temp_df['yoy_weight']\n",
    "    temp_df = pd.pivot_table(temp_df.reset_index(), values='yoy', index='Date', columns=cat_col)\n",
    "    cpi_yoy_df = pd.concat([cpi_yoy_df, temp_df], axis=1)\n",
    "\n",
    "# calculate (Western) core Flexible and Sticky\n",
    "temp_df = df_cpi_all[df_cpi_all['WesternCore']==1]\n",
    "temp_df = temp_df.groupby(['Date','Atlanta'])[['yoy_weight','yoy:weight']].sum()\n",
    "temp_df['yoy'] = temp_df['yoy:weight']/temp_df['yoy_weight']\n",
    "temp_df = pd.pivot_table(temp_df.reset_index(), values='yoy', index='Date', columns='Atlanta')\n",
    "temp_df.columns = [f'WesternCore {x}' for x in temp_df.columns]\n",
    "cpi_yoy_df = pd.concat([cpi_yoy_df, temp_df], axis=1)\n",
    "\n",
    "# calculate (Western) core Flexible and Sticky\n",
    "temp_df = df_cpi_all[df_cpi_all['BoJCore']==1]\n",
    "temp_df = temp_df.groupby(['Date','Atlanta'])[['yoy_weight','yoy:weight']].sum()\n",
    "temp_df['yoy'] = temp_df['yoy:weight']/temp_df['yoy_weight']\n",
    "temp_df = pd.pivot_table(temp_df.reset_index(), values='yoy', index='Date', columns='Atlanta')\n",
    "temp_df.columns = [f'BoJCore {x}' for x in temp_df.columns]\n",
    "cpi_yoy_df = pd.concat([cpi_yoy_df, temp_df], axis=1)\n",
    "\n",
    "# calculate flexible and sticky inflation by type\n",
    "temp_df = df_cpi_all.groupby(['Date','Atlanta','Type'])[['yoy_weight','yoy:weight']].sum()\n",
    "temp_df['yoy'] = temp_df['yoy:weight']/temp_df['yoy_weight']\n",
    "temp_df = pd.pivot_table(temp_df.reset_index(), values='yoy', index='Date', columns=['Atlanta','Type'])\n",
    "temp_df.columns = [f'{x[0]}:{x[1]}' for x in temp_df.columns]\n",
    "cpi_yoy_df = pd.concat([cpi_yoy_df, temp_df], axis=1)\n",
    "\n",
    "cpi_yoy_df.to_csv('./特殊要因除くCPI前年比.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93063f43",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# calculate m/m\n",
    "# first create item-level index with base year of 2008 (since there's no yoy adjustment in 2018)\n",
    "# use 1980\n",
    "index_baseMonth = datetime(2008,1,1)\n",
    "baseMonth_loc = list(cpi_dfs['index'].index).index(index_baseMonth)\n",
    "\n",
    "# aggregate\n",
    "df_cpi_all['index:weight'] = df_cpi_all['index']*df_cpi_all['index_weight']\n",
    "cpi_index_df = pd.DataFrame(index=cpi_dfs['index'].index)\n",
    "\n",
    "# Overall/Core/BoJ core/CPI ex imputed rents and FF\n",
    "cal_cols = [\n",
    "    'Overall',\n",
    "    'Core',\n",
    "    'BoJCore',\n",
    "    'WesternCore',\n",
    "    'CPIExImpRents',\n",
    "    'CPIExImpRents&FF',\n",
    "    'GoodsExFFE',\n",
    "    'GenSerExRents'\n",
    "]\n",
    "\n",
    "for col in cal_cols:\n",
    "    temp_df = df_cpi_all.copy()[df_cpi_all[col] == 1]\n",
    "    temp_agg_index = temp_df.groupby(['Date'])[['index_weight','index:weight']].sum()\n",
    "    temp_agg_index[col] = temp_agg_index['index:weight']/temp_agg_index['index_weight']\n",
    "    cpi_index_df[col] = temp_agg_index[col]\n",
    "\n",
    "\n",
    "# aggregate by category\n",
    "cat_cols = ['Type','Atlanta']\n",
    "for cat_col in cat_cols:\n",
    "    temp_df = df_cpi_all.groupby(['Date',cat_col])[['index_weight','index:weight']].sum()\n",
    "    temp_df['index'] = temp_df['index:weight']/temp_df['index_weight']\n",
    "    temp_df = pd.pivot_table(temp_df.reset_index(), values='index', index='Date', columns=cat_col)\n",
    "    cpi_index_df = pd.concat([cpi_index_df, temp_df], axis=1)\n",
    "\n",
    "# calculate (Western) core Flexible and Sticky\n",
    "temp_df = df_cpi_all[df_cpi_all['WesternCore']==1]\n",
    "temp_df = temp_df.groupby(['Date','Atlanta'])[['index_weight','index:weight']].sum()\n",
    "temp_df['index'] = temp_df['index:weight']/temp_df['index_weight']\n",
    "temp_df = pd.pivot_table(temp_df.reset_index(), values='index', index='Date', columns='Atlanta')\n",
    "temp_df.columns = [f'WesternCore {x}' for x in temp_df.columns]\n",
    "cpi_index_df = pd.concat([cpi_index_df, temp_df], axis=1)\n",
    "\n",
    "# calculate (Western) core Flexible and Sticky\n",
    "temp_df = df_cpi_all[df_cpi_all['BoJCore']==1]\n",
    "temp_df = temp_df.groupby(['Date','Atlanta'])[['index_weight','index:weight']].sum()\n",
    "temp_df['index'] = temp_df['index:weight']/temp_df['index_weight']\n",
    "temp_df = pd.pivot_table(temp_df.reset_index(), values='index', index='Date', columns='Atlanta')\n",
    "temp_df.columns = [f'BoJCore {x}' for x in temp_df.columns]\n",
    "cpi_index_df = pd.concat([cpi_index_df, temp_df], axis=1)\n",
    "\n",
    "# calculate flexible and sticky inflation by type\n",
    "temp_df = df_cpi_all.groupby(['Date','Atlanta','Type'])[['index_weight','index:weight']].sum()\n",
    "temp_df['index'] = temp_df['index:weight']/temp_df['index_weight']\n",
    "temp_df = pd.pivot_table(temp_df.reset_index(), values='index', index='Date', columns=['Atlanta','Type'])\n",
    "temp_df.columns = [f'{x[0]}:{x[1]}' for x in temp_df.columns]\n",
    "cpi_index_df = pd.concat([cpi_index_df, temp_df], axis=1)\n",
    "\n",
    "def calc_index_adj(index_array, yoy_adj_array, index_baseMonth=index_baseMonth):\n",
    "    # print(index_array.shape, yoy_adj_array.shape)\n",
    "    baseMonth_loc = list(cpi_index_df.index).index(index_baseMonth)\n",
    "    temp_array = np.zeros(len(cpi_index_df))\n",
    "\n",
    "    for ix in range(baseMonth_loc, baseMonth_loc+12):\n",
    "        temp_array[ix] = index_array[ix]\n",
    "    for ix in range(baseMonth_loc+12, len(temp_array)):\n",
    "        temp_array[ix] = temp_array[ix-12]*(1+yoy_adj_array[ix-12]/100)\n",
    "    for ix in reversed(range(baseMonth_loc)):\n",
    "        temp_array[ix] = temp_array[ix+12]/(1+yoy_adj_array[ix]/100)\n",
    "\n",
    "    return temp_array\n",
    "\n",
    "for col in cpi_index_df.columns:\n",
    "    cpi_index_df[f'{col}_adj'] = calc_index_adj(cpi_index_df[col].values, cpi_yoy_df[col].values)\n",
    "\n",
    "\n",
    "index_base = np.nanmean(cpi_index_df[(cpi_index_df.index >= datetime(2020,1,1))&(cpi_index_df.index <= datetime(2020,12,31))], axis=0)\n",
    "cpi_index_df = cpi_index_df/index_base*100\n",
    "cpi_index_df = cpi_index_df[[x for x in cpi_index_df.columns if x.endswith('_adj')]]\n",
    "cpi_index_df.columns = [x.replace('_adj', '') for x in cpi_index_df.columns]\n",
    "cpi_index_df.to_csv('./集計指数NSA.csv')\n",
    "\n",
    "# # seasonal adjustment will be done on Eviews (the following code not working)\n",
    "# x12path = r\"C:\\Users\\x01494487\\Desktop\\Python\\cpi_dl\\x13as_ascii-v1-1-b61\\x13as\"\n",
    "# cpi_index_sa_df = cpi_index_df.copy()\n",
    "# for col in tqdm(cpi_index_sa_df.columns):\n",
    "#     cpi_index_sa_df = sm.tsa.x13_arima_analysis(cpi_index_df[col], x12path=x12path, prefer_x13=True).seasadj\n",
    "\n",
    "# load sa series\n",
    "# cpi_index_sa_df = pd.read_excel('./集計指数SA.xlsx', sheet_name='data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14346fe5",
   "metadata": {},
   "source": [
    "# Monthly Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9e6864",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#DL (BBG)\n",
    "tickerList = pd.read_excel('./tickerList.xlsx', sheet_name='tickerList')\n",
    "dfs = {}\n",
    "dlItems = tickerList.columns[1:]\n",
    "dt = datetime.today()\n",
    "start_date = \"2020-01-01\"\n",
    "end_date = dt.replace(day = calendar.monthrange(dt.year, dt.month)[1])\n",
    "end_date = f'{end_date.year}-{str(end_date.month).zfill(2)}-{end_date.day}'\n",
    "for dlItem in tqdm(dlItems):\n",
    "    temp_df = blp.bdh(\n",
    "        tickers=tickerList[dlItem],\n",
    "        flds = [\"PX_LAST\"],\n",
    "        start_date = start_date,\n",
    "        end_date = end_date,\n",
    "        **kwargs\n",
    "        )\n",
    "\n",
    "    # 浄化槽清掃代（除いてもいいが列数を揃えたいのでごちゃごちゃやる）\n",
    "    if 't_' in dlItem:\n",
    "        temp_df.columns = [x for x in tickerList['cpi_item'] if x!= '浄化槽清掃代']\n",
    "        temp_df['浄化槽清掃代'] = 0\n",
    "    else:\n",
    "        temp_df.columns = tickerList['cpi_item']\n",
    "    temp_df = temp_df[tickerList['cpi_item']] #改めて並び替える\n",
    "    dfs[dlItem] = temp_df\n",
    "\n",
    "dfs['n_index_3digit'] = df_n_3dgit\n",
    "dfs['n_yoy_3digit'] = df_n_3dgit.pct_change(periods=12, fill_method=None)*100\n",
    "dfs['n_yoy_manual'] = dfs['n_index'].pct_change(periods=12, fill_method=None)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c754fd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#DL (BBG: Aggregates)\n",
    "tickerList = pd.read_excel('./tickerList_agg.xlsx', sheet_name='tickerList')\n",
    "dlItems = tickerList.columns[1:]\n",
    "dt = datetime.today()\n",
    "start_date = \"2020-01-01\"\n",
    "end_date = dt.replace(day = calendar.monthrange(dt.year, dt.month)[1])\n",
    "end_date = f'{end_date.year}-{str(end_date.month).zfill(2)}-{end_date.day}'\n",
    "for dlItem in tqdm(dlItems):\n",
    "    temp_df = blp.bdh(\n",
    "        tickers=tickerList[dlItem].dropna(),\n",
    "        flds = [\"PX_LAST\"],\n",
    "        start_date = start_date,\n",
    "        end_date = end_date,\n",
    "        **kwargs\n",
    "        )\n",
    "    dfs[dlItem] = temp_df\n",
    "    temp_df.columns = tickerList['cpi_item'][-len(tickerList[dlItem].dropna()):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4122c6e6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#save\n",
    "with pd.ExcelWriter('./cpi_2020.xlsx') as writer:\n",
    "    for df_name, df in dfs.items():\n",
    "        df.to_excel(writer, sheet_name=df_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce86bff0",
   "metadata": {},
   "source": [
    "# High Labor Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfa490f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Nationide\n",
    "dfs = pd.read_excel('./cpi_2020.xlsx', engine=\"openpyxl\", sheet_name=None, index_col=0)\n",
    "weight_dict = pd.read_excel('./weights_flags.xlsx', engine=\"openpyxl\", sheet_name=None)\n",
    "calc_dict ={\n",
    "    '全体':'HLS',\n",
    "    '外食':'HLS/EatOut',\n",
    "    '家事':'HLS/House',\n",
    "    '医療・福祉':'HLS/Med',\n",
    "    '教育':'HLS/Edu',\n",
    "    '通信・教養娯楽':'HLS/TelRec',\n",
    "    '低人件費率':'LLS'\n",
    "}\n",
    "\n",
    "df_hls_n = pd.DataFrame()\n",
    "for key, val in calc_dict.items():\n",
    "    temp_weight = (weight_dict['N_2020'].values * weight_dict['Flags'][val].fillna(0).values)\n",
    "\n",
    "    hl_weight = np.zeros(dfs['n_index'].shape)\n",
    "    for ix in range(hl_weight.shape[0]):\n",
    "        hl_weight[ix] = temp_weight[dfs['n_index'].index[ix].month - 1]\n",
    "        hl_weight[ix] = hl_weight[ix]/hl_weight[ix].sum()\n",
    "\n",
    "    df_hls_n[key] = (np.nan_to_num(dfs['n_index'].values) * hl_weight).sum(axis=1)\n",
    "\n",
    "# Tokyo\n",
    "df_hls_t = pd.DataFrame()\n",
    "for key, val in calc_dict.items():\n",
    "    temp_weight = (weight_dict['T_2020'].values * weight_dict['Flags'][val].fillna(0).values)\n",
    "\n",
    "    hl_weight = np.zeros(dfs['t_index'].shape)\n",
    "    for ix in range(hl_weight.shape[0]):\n",
    "        hl_weight[ix] = temp_weight[dfs['t_index'].index[ix].month - 1]\n",
    "        hl_weight[ix] = hl_weight[ix]/hl_weight[ix].sum()\n",
    "\n",
    "    df_hls_t[key] = (np.nan_to_num(dfs['t_index'].values) * hl_weight).sum(axis=1)\n",
    "\n",
    "\n",
    "#save\n",
    "save_dir = r'\\\\intranet.barcapint.com\\dfs-apac\\Group\\TKY\\ops\\economics\\Hashimoto\\06_Commentary\\CPI\\hls'\n",
    "df_hls_n.to_csv(f'{save_dir}/hls_n.csv', index=False,encoding='shift-jis')\n",
    "df_hls_t.to_csv(f'{save_dir}/hls_t.csv', index=False,encoding='shift-jis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a97b5d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# hls flag\n",
    "hls_flags = weight_dict['Flags']\n",
    "\n",
    "# calculate kernel density (nationwide)\n",
    "df_wide = dfs['n_yoy'].reset_index()\n",
    "df_long = df_wide.melt(id_vars=['index']).dropna()\n",
    "df_long.columns = ['Date','Item','YoY']\n",
    "df_long = df_long.merge(hls_flags, how='left', on='Item')\n",
    "df_long['Year'] = [x.year for x in df_long['Date']]\n",
    "df_long['YearQ'] = [f'{x.year}Q{(x.month-1)//3+1}' for x in df_long['Date']]\n",
    "df_long = df_long[df_long['HLS'].isin([0,1])]\n",
    "\n",
    "# Yearly\n",
    "p = itertools.product(set(df_long['HLS']), sorted(set(df_long['Year'])))\n",
    "\n",
    "df_results = pd.DataFrame()\n",
    "x_min = -4\n",
    "x_max = +10\n",
    "x = np.arange(x_min, x_max+0.2, 0.2)\n",
    "df_results['x'] = x\n",
    "x = x.reshape(x.shape[0], 1)\n",
    "\n",
    "for v in p:\n",
    "    df_temp = df_long[(df_long['HLS']==v[0])&(df_long['Year']==v[1])]\n",
    "    yoy_array = np.array(df_temp['YoY'])\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=0.5).fit(yoy_array.reshape(yoy_array.shape[0], 1))\n",
    "    log_density = kde.score_samples(x)\n",
    "    df_results[v] = np.exp(log_density)\n",
    "\n",
    "df_results.to_csv(f'{save_dir}/hls_n_kd_y.csv', index=False)\n",
    "\n",
    "# Quarterly\n",
    "p = itertools.product(set(df_long['HLS']), sorted(set(df_long['YearQ'])))\n",
    "\n",
    "df_results = pd.DataFrame()\n",
    "x_min = -4\n",
    "x_max = +10\n",
    "x = np.arange(x_min, x_max+0.2, 0.2)\n",
    "df_results['x'] = x\n",
    "x = x.reshape(x.shape[0], 1)\n",
    "\n",
    "for v in p:\n",
    "    df_temp = df_long[(df_long['HLS']==v[0])&(df_long['YearQ']==v[1])]\n",
    "    yoy_array = np.array(df_temp['YoY'])\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=0.5).fit(yoy_array.reshape(yoy_array.shape[0], 1))\n",
    "    log_density = kde.score_samples(x)\n",
    "    df_results[v] = np.exp(log_density)\n",
    "\n",
    "df_results.to_csv(f'{save_dir}/hls_n_kd_q.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8507a1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# calculate kernel density (Tokyo)\n",
    "df_wide = dfs['t_yoy'].reset_index()\n",
    "df_long = df_wide.melt(id_vars=['index']).dropna()\n",
    "df_long.columns = ['Date','Item','YoY']\n",
    "df_long = df_long.merge(hls_flags, how='left', on='Item')\n",
    "df_long['Year'] = [x.year for x in df_long['Date']]\n",
    "df_long['YearQ'] = [f'{x.year}Q{(x.month-1)//3+1}' for x in df_long['Date']]\n",
    "df_long = df_long[df_long['HLS'].isin([0,1])]\n",
    "\n",
    "# Yearly\n",
    "p = itertools.product(set(df_long['HLS']), sorted(set(df_long['Year'])))\n",
    "\n",
    "df_results = pd.DataFrame()\n",
    "x_min = -4\n",
    "x_max = +10\n",
    "x = np.arange(x_min, x_max+0.2, 0.2)\n",
    "df_results['x'] = x\n",
    "x = x.reshape(x.shape[0], 1)\n",
    "\n",
    "for v in p:\n",
    "    df_temp = df_long[(df_long['HLS']==v[0])&(df_long['Year']==v[1])]\n",
    "    yoy_array = np.array(df_temp['YoY'])\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=0.5).fit(yoy_array.reshape(yoy_array.shape[0], 1))\n",
    "    log_density = kde.score_samples(x)\n",
    "    df_results[v] = np.exp(log_density)\n",
    "\n",
    "df_results.to_csv(f'{save_dir}/hls_t_kd_y.csv', index=False)\n",
    "\n",
    "# Quarterly\n",
    "p = itertools.product(set(df_long['HLS']), sorted(set(df_long['YearQ'])))\n",
    "\n",
    "df_results = pd.DataFrame()\n",
    "x_min = -4\n",
    "x_max = +10\n",
    "x = np.arange(x_min, x_max+0.2, 0.2)\n",
    "df_results['x'] = x\n",
    "x = x.reshape(x.shape[0], 1)\n",
    "\n",
    "for v in p:\n",
    "    df_temp = df_long[(df_long['HLS']==v[0])&(df_long['YearQ']==v[1])]\n",
    "    yoy_array = np.array(df_temp['YoY'])\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=0.5).fit(yoy_array.reshape(yoy_array.shape[0], 1))\n",
    "    log_density = kde.score_samples(x)\n",
    "    df_results[v] = np.exp(log_density)\n",
    "\n",
    "df_results.to_csv(f'{save_dir}/hls_t_kd_q.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac901c0b",
   "metadata": {},
   "source": [
    "# Underlying inflation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f80758",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# # load data\n",
    "dfs = pd.read_excel('./cpi_2020.xlsx', engine=\"openpyxl\", sheet_name=None, index_col=0)\n",
    "weight_dict = pd.read_excel('./weights_flags.xlsx', engine=\"openpyxl\", sheet_name=None)\n",
    "Items = weight_dict['Flags']\n",
    "CoreItems = Items[Items['Core']==1]['Item']\n",
    "\n",
    "yoy = dfs['n_yoy_manual']\n",
    "yoy = yoy[yoy.index >= '2022-01-01']\n",
    "yoy_core = yoy[list(CoreItems)]\n",
    "yoy_official = dfs['n_yoy']\n",
    "yoy_official = yoy_official[yoy_official.index >= '2022-01-01']\n",
    "yoy_official_core = yoy_official[list(CoreItems)]\n",
    "\n",
    "weight_array = yoy.copy().values\n",
    "weight_core_array = yoy_official_core.copy().values\n",
    "for row in range(len(weight_array)):\n",
    "    weight_array[row,:] = weight_dict['N_2020'].values[row % 12]\n",
    "    temp_weight_df = weight_dict['N_2020'][list(CoreItems)]\n",
    "    weight_core_array[row,:] = temp_weight_df.values[row % 12]\n",
    "\n",
    "yoy_array = yoy.values\n",
    "yoy_core_array = yoy_core.values\n",
    "yoy_official_array = yoy_official.values\n",
    "yoy_official_core_array = yoy_official_core.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830b64b2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def weighted_quantile(values, quantiles, sample_weight=None,\n",
    "                      values_sorted=False, old_style=False):\n",
    "    \"\"\" Very close to numpy.percentile, but supports weights.\n",
    "    NOTE: quantiles should be in [0, 1]!\n",
    "    :param values: numpy.array with data\n",
    "    :param quantiles: array-like with many quantiles needed\n",
    "    :param sample_weight: array-like of the same length as `array`\n",
    "    :param values_sorted: bool, if True, then will avoid sorting of\n",
    "        initial array\n",
    "    :param old_style: if True, will correct output to be consistent\n",
    "        with numpy.percentile.\n",
    "    :return: numpy.array with computed quantiles.\n",
    "    \"\"\"\n",
    "    values = np.array(values)\n",
    "    quantiles = np.array(quantiles)\n",
    "    if sample_weight is None:\n",
    "        sample_weight = np.ones(len(values))\n",
    "    sample_weight = np.array(sample_weight)\n",
    "    assert np.all(quantiles >= 0) and np.all(quantiles <= 1), \\\n",
    "        'quantiles should be in [0, 1]'\n",
    "\n",
    "    if not values_sorted:\n",
    "        sorter = np.argsort(values)\n",
    "        values = values[sorter]\n",
    "        sample_weight = sample_weight[sorter]\n",
    "\n",
    "    # weighted_quantiles = np.cumsum(sample_weight) - 0.5 * sample_weight\n",
    "    weighted_quantiles = np.cumsum(sample_weight)\n",
    "    if old_style:\n",
    "        # To be convenient with numpy.percentile\n",
    "        weighted_quantiles -= weighted_quantiles[0]\n",
    "        weighted_quantiles /= weighted_quantiles[-1]\n",
    "    else:\n",
    "        weighted_quantiles /= np.sum(sample_weight)\n",
    "    return np.interp(quantiles, weighted_quantiles, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34f1569",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "w_median = []\n",
    "# w_median_bbg = []\n",
    "t_mean = []\n",
    "# t_mean_bbg = []\n",
    "mode = []\n",
    "for i in range(yoy_official_core_array.shape[0]):\n",
    "    # temp_yoy = yoy_array[i,:][~np.isnan(yoy_array[i,:])]\n",
    "    temp_yoy = yoy_core_array[i,:][~np.isnan(yoy_core_array[i,:])]\n",
    "    # temp_yoy = yoy_official_core_array[i,:][~np.isnan(yoy_official_core_array[i,:])]\n",
    "    temp_weight = weight_core_array[i,:][np.nonzero(weight_core_array[i,:])]\n",
    "    wqtile = weighted_quantile(temp_yoy, quantiles=[0.1, 0.475, 0.5, 0.525, 0.9], sample_weight=temp_weight)\n",
    "    v1 = wqtile[2]\n",
    "    v2 = (wqtile[1]+wqtile[3])/2\n",
    "    v3 = temp_yoy[(temp_yoy >= wqtile[1]) & (temp_yoy <= wqtile[3])].mean()\n",
    "    v4 = np.average(temp_yoy[(temp_yoy >= wqtile[1]) & (temp_yoy <= wqtile[3])], weights=temp_weight[(temp_yoy >= wqtile[1]) & (temp_yoy <= wqtile[3])])\n",
    "    # w_median.append(wqtile[2])\n",
    "    # w_median.append((wqtile[1]+wqtile[3])/2)\n",
    "    # w_median.append(temp_yoy[(temp_yoy >= wqtile[1]) & (temp_yoy <= wqtile[3])].mean())\n",
    "    w_median.append(v2)\n",
    "    # trimmed_array = (temp_yoy >= wqtile[1]) & (temp_yoy <= wqtile[3])\n",
    "    # w_median.append(np.average(temp_yoy[trimmed_array], weights=temp_weight[trimmed_array]))\n",
    "\n",
    "\n",
    "    # temp_yoy = yoy_official_core_array[i,:][~np.isnan(yoy_official_core_array[i,:])]\n",
    "    trimmed_array = (temp_yoy >= wqtile[0]) & (temp_yoy <= wqtile[4])\n",
    "    # print(i, trimmed_array.shape, temp_yoy.shape, temp_weight.shape)\n",
    "    temp_t_mean = np.average(temp_yoy[trimmed_array], weights=temp_weight[trimmed_array])\n",
    "    t_mean.append(temp_t_mean)\n",
    "\n",
    "    # temp_yoy_bbg = yoy_bbg_array[i,:][~np.isnan(yoy_bbg_array[i,:])]\n",
    "    # wqtile_bbg = weighted_quantile(temp_yoy_bbg, quantiles=[0.1, 0.475, 0.5, 0.525, 0.9], sample_weight=temp_weight)\n",
    "    # w_median_bbg.append(wqtile_bbg[2])\n",
    "\n",
    "    # trimmed_bbg_array = (temp_yoy > wqtile_bbg[0]) & (temp_yoy < wqtile_bbg[4])\n",
    "    # temp_t_mean_bbg = np.average(temp_yoy_bbg[trimmed_bbg_array], weights=temp_weight[trimmed_bbg_array])\n",
    "    # t_mean_bbg.append(temp_t_mean_bbg)\n",
    "\n",
    "    # temp_yoy_manual = yoy_array[i,:][~np.isnan(yoy_array[i,:])]\n",
    "    # a1, b1, loc1, scale1 = norminvgauss.fit(temp_yoy_manual)\n",
    "    temp_yoy_official = yoy_official_core_array[i,:][~np.isnan(yoy_official_core_array[i,:])]\n",
    "    # temp_yoy_official = temp_yoy_official[(temp_yoy_official>=-20) & (temp_yoy_official<=30)]\n",
    "    a1, b1, loc1, scale1 = norminvgauss.fit(temp_yoy_official)\n",
    "    if i == 0:\n",
    "        a1, b1, loc1, scale1 = norminvgauss.fit(temp_yoy_official)\n",
    "    else:\n",
    "        a1, b1, loc1, scale1 = norminvgauss.fit(temp_yoy_official, loc=loc1, scale=scale1)\n",
    "    x = np.arange(-20, 50+1, 0.1)\n",
    "    pdf_array = norminvgauss.pdf(x, a1, b1, loc1, scale1)\n",
    "    mode.append(x[pdf_array.argmax()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775f064d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame({\n",
    "    'w_median':w_median,\n",
    "    # 'w_median_bbg':w_median_bbg,\n",
    "    't_mean':t_mean,\n",
    "    # 't_mean_bbg':t_mean_bbg\n",
    "    'mode':mode\n",
    "})\n",
    "\n",
    "#save\n",
    "save_dir = r'\\\\intranet.barcapint.com\\dfs-apac\\Group\\TKY\\ops\\economics\\Hashimoto\\06_Commentary\\CPI\\underlying'\n",
    "result_df.to_csv(f'{save_dir}/results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edafd1fa",
   "metadata": {},
   "source": [
    "# Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60cd34e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# # load data\n",
    "dfs = pd.read_excel('./cpi_2020.xlsx', engine=\"openpyxl\", sheet_name=None, index_col=0)\n",
    "sheets = ['n_yoycont','n_momcont','n_yoy_cont_agg','n_mom_cont_agg','t_yoycont','t_momcont','t_yoy_cont_agg','t_mom_cont_agg']\n",
    "\n",
    "# choose last 15 months, transpose, then save as csv\n",
    "save_dir = r'\\\\intranet.barcapint.com\\dfs-apac\\Group\\TKY\\ops\\economics\\Hashimoto\\06_Commentary\\CPI\\contribution'\n",
    "\n",
    "for sheet in tqdm(sheets):\n",
    "    df_temp = dfs[sheet].tail(15)\n",
    "    df_temp.sort_index(ascending=False).T.to_csv(f'{save_dir}/{sheet}.csv', encoding='shift-jis')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
